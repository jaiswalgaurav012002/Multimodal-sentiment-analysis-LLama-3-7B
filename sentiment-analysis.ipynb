{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118722b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- CACHE REDIRECTION (Must be FIRST) -------------------\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"F:/GUARAV 2/huggingface\"\n",
    "os.environ[\"HF_HUB_CACHE\"] = \"F:/GUARAV 2/huggingface/hub\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"F:/GUARAV 2/huggingface/transformers\"\n",
    "\n",
    "# ------------------- SYSTEM -------------------\n",
    "import sys\n",
    "import warnings\n",
    "import subprocess\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# ‚úÖ DEVICE AUTO-DETECT\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üß† Using device: {DEVICE}\")\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "if DEVICE == \"cpu\":\n",
    "    print(\"‚ö†Ô∏è CUDA not available. Running on CPU.\")\n",
    "else:\n",
    "    print(f\"‚úÖ CUDA detected: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# ------------------- DATA SCIENCE -------------------\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------- AUDIO / SPEECH -------------------\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from pydub import AudioSegment\n",
    "from pydub.utils import which\n",
    "AudioSegment.converter = which(\"ffmpeg\") or r\"F:\\\\ffmpeg\\\\bin\\\\ffmpeg.exe\"\n",
    "\n",
    "from noisereduce import reduce_noise\n",
    "import parselmouth\n",
    "\n",
    "# ------------------- HUGGING FACE TRANSFORMERS -------------------\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoFeatureExtractor,\n",
    "    Wav2Vec2ForSequenceClassification,\n",
    "    RagRetriever,\n",
    "    RagTokenizer,\n",
    "    RagSequenceForGeneration,\n",
    ")\n",
    "\n",
    "# ------------------- WHISPER -------------------\n",
    "import whisper\n",
    "\n",
    "# ------------------- FAISS (GPU/CPU) -------------------\n",
    "import faiss\n",
    "\n",
    "# ------------------- SILERO VAD (PATCHED) -------------------\n",
    "try:\n",
    "    model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', trust_repo=True)\n",
    "    (get_speech_timestamps, _, _, _, _) = utils\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Silero VAD load failed: {e}\")\n",
    "    get_speech_timestamps = None\n",
    "\n",
    "# ------------------- OPTIONAL: NLTK -------------------\n",
    "try:\n",
    "    import nltk\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è NLTK not found. Some NLP steps may be limited.\")\n",
    "\n",
    "# ------------------- ENV LOADER -------------------\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# ------------------- SPA & NLP -------------------\n",
    "import spacy\n",
    "from num2words import num2words\n",
    "\n",
    "# ------------------- TTS CONFIG -------------------\n",
    "from bark import generate_audio  # ‚úÖ Bark TTS entrypoint\n",
    "from bark.generation import preload_models\n",
    "\n",
    "USE_TTS = True          # üîä Use Bark TTS if available\n",
    "USE_TTS_LITE = True     # üîÅ Use pyttsx3 if Bark fails\n",
    "USE_TTS_ALT = True      # üîÅ Use gTTS if pyttsx3 also fails\n",
    "\n",
    "# ------------------- HERMES LOADER -------------------\n",
    "def load_hermes_model():\n",
    "    os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "    preferred_model = \"teknium/OpenHermes-2.5-Mistral-7B\"  # ‚úÖ OPEN access\n",
    "    fallback_model_1 = \"mistralai/Mistral-7B-Instruct-v0.3\"  # üîí Gated\n",
    "    fallback_model_2 = \"NousResearch/Nous-Hermes-2-Mistral-7B\"  # üîí Gated\n",
    "\n",
    "    try:\n",
    "        print(f\"üîÑ Loading tokenizer for {preferred_model}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(preferred_model)\n",
    "        print(f\"üì¶ Loading model: {preferred_model}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(preferred_model).to(DEVICE)\n",
    "        print(\"‚úÖ OpenHermes-2.5-Mistral-7B loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to load {preferred_model}: {e}\")\n",
    "        try:\n",
    "            print(\"üîÅ Falling back to Mistral-7B-Instruct-v0.3...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(fallback_model_1)\n",
    "            model = AutoModelForCausalLM.from_pretrained(fallback_model_1).to(DEVICE)\n",
    "            print(\"‚úÖ Fallback Mistral-7B-Instruct-v0.3 loaded.\")\n",
    "        except Exception as e1:\n",
    "            print(f\"‚ö†Ô∏è Failed to load fallback_model_1: {e1}\")\n",
    "            try:\n",
    "                print(\"üîÅ Falling back to Nous-Hermes-2-Mistral-7B...\")\n",
    "                tokenizer = AutoTokenizer.from_pretrained(fallback_model_2)\n",
    "                model = AutoModelForCausalLM.from_pretrained(fallback_model_2).to(DEVICE)\n",
    "                print(\"‚úÖ Fallback Nous-Hermes-2-Mistral-7B loaded.\")\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ùå All Hermes model loading attempts failed: {e2}\")\n",
    "                raise RuntimeError(\"üö´ No Hermes-based model could be loaded. Please check access.\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "# ------------------- RAG RETRIEVER -------------------\n",
    "from datasets import load_dataset\n",
    "\n",
    "def load_rag_components():\n",
    "    print(\"üìö Loading wiki_dpr dataset with trust_remote_code=True...\")\n",
    "    try:\n",
    "        rag_dataset = load_dataset(\"wiki_dpr\", trust_remote_code=True)\n",
    "        print(\"‚úÖ wiki_dpr dataset loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load wiki_dpr: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    try:\n",
    "        print(\"üì¶ Loading RAG components...\")\n",
    "        rag_tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "        rag_retriever = RagRetriever.from_pretrained(\n",
    "            \"facebook/rag-token-base\",\n",
    "            index_name=\"legacy\",\n",
    "            use_dummy_dataset=True\n",
    "        )\n",
    "        rag_model = RagSequenceForGeneration.from_pretrained(\"facebook/rag-token-base\").to(DEVICE)\n",
    "        print(\"‚úÖ RAG components ready.\")\n",
    "        return rag_tokenizer, rag_retriever, rag_model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå RAG model loading failed: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# ------------------- FALCON 3 - 7B LOADER -------------------\n",
    "def setup_falcon_cache_on_f_drive():\n",
    "    base_dir = \"F:/huggingface\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_dir, \"hub\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_dir, \"transformers\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(base_dir, \"logs\"), exist_ok=True)\n",
    "\n",
    "    os.environ[\"HF_HOME\"] = base_dir\n",
    "    os.environ[\"HF_HUB_CACHE\"] = os.path.join(base_dir, \"hub\")\n",
    "    os.environ[\"TRANSFORMERS_CACHE\"] = os.path.join(base_dir, \"transformers\")\n",
    "\n",
    "    print(\"üìÅ Falcon cache configured:\")\n",
    "    print(f\"   HF_HOME = {os.environ['HF_HOME']}\")\n",
    "    print(f\"   HF_HUB_CACHE = {os.environ['HF_HUB_CACHE']}\")\n",
    "    print(f\"   TRANSFORMERS_CACHE = {os.environ['TRANSFORMERS_CACHE']}\")\n",
    "\n",
    "def load_falcon3_model():\n",
    "    model_id = \"tiiuae/Falcon3-7B-Base\"\n",
    "    setup_falcon_cache_on_f_drive()\n",
    "\n",
    "    try:\n",
    "        print(f\"üîÑ Downloading Falcon3 Tokenizer: {model_id}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        print(f\"üì¶ Downloading Falcon3 Model: {model_id}\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id).to(DEVICE)\n",
    "        print(\"‚úÖ Falcon3-7B-Base loaded.\")\n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Falcon3 loading failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# ------------------- ACTIVE MODELS -------------------\n",
    "llm_tokenizer, llm_model = load_hermes_model()  # ‚úÖ\n",
    "falcon3_tokenizer, falcon3_model = load_falcon3_model()\n",
    "rag_tokenizer, rag_retriever, rag_model = load_rag_components()\n",
    "\n",
    "ACTIVE_MODELS = {\n",
    "    \"hermes\": {\"tokenizer\": llm_tokenizer, \"model\": llm_model},\n",
    "    \"falcon3\": {\"tokenizer\": falcon3_tokenizer, \"model\": falcon3_model},\n",
    "    \"rag\": {\"tokenizer\": rag_tokenizer, \"retriever\": rag_retriever, \"model\": rag_model},\n",
    "}\n",
    "\n",
    "# ------------------- STEP 3: Preprocessing (Speech + Text + Emotion) -------------------\n",
    "\n",
    "import re\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoFeatureExtractor,\n",
    "    Wav2Vec2ForSequenceClassification\n",
    ")\n",
    "\n",
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "import spacy\n",
    "\n",
    "# ‚úÖ Global device selection\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"üñ•Ô∏è Step 3 running on: {DEVICE.upper()}\")\n",
    "\n",
    "# üîπ Speaker Diarization\n",
    "diarization_pipeline = SpeakerDiarization.from_pretrained(\n",
    "    \"pyannote/speaker-diarization\",\n",
    "    use_auth_token=os.getenv(\"HF_TOKEN\")\n",
    ")\n",
    "\n",
    "# üîπ SpaCy NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# üîπ Sentiment & Emotion (Text)\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", device=0 if DEVICE == \"cuda\" else -1)\n",
    "emotion_pipeline = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True, device=0 if DEVICE == \"cuda\" else -1)\n",
    "\n",
    "# üîπ Emotion from Speech\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"superb/wav2vec2-large-superb-er\")\n",
    "speech_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-large-superb-er\").to(DEVICE)\n",
    "\n",
    "# ------------------- AUDIO PREPROCESSING -------------------\n",
    "\n",
    "def preprocess_speech(audio_path):\n",
    "    waveform, _ = librosa.load(audio_path, sr=16000)\n",
    "    speech_timestamps = get_speech_timestamps(waveform, sampling_rate=16000)\n",
    "\n",
    "    if not speech_timestamps:\n",
    "        print(\"‚ö†Ô∏è No speech detected by VAD.\")\n",
    "        return {\"waveform\": waveform, \"diarization\": None}\n",
    "\n",
    "    # Extract voiced parts\n",
    "    voiced_segments = np.concatenate([waveform[seg['start']:seg['end']] for seg in speech_timestamps])\n",
    "\n",
    "    diarization = diarization_pipeline({\"uri\": \"sample\", \"waveform\": waveform})\n",
    "\n",
    "    return {\n",
    "        \"waveform\": voiced_segments,\n",
    "        \"diarization\": diarization\n",
    "    }\n",
    "\n",
    "# ------------------- TEXT PREPROCESSING + NER -------------------\n",
    "\n",
    "def expand_contractions(text):\n",
    "    contractions = {\"i'm\": \"i am\", \"you're\": \"you are\", \"it's\": \"it is\"}\n",
    "    return \" \".join([contractions.get(word.lower(), word) for word in text.split()])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = expand_contractions(text.lower())\n",
    "    text = re.sub(r\"\\d+\", lambda x: num2words(x.group()), text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return {\n",
    "        \"cleaned_text\": text,\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "# ------------------- TEXT-BASED SENTIMENT + EMOTION -------------------\n",
    "\n",
    "def analyze_text_sentiment(text):\n",
    "    sentiment = sentiment_pipeline(text)[0][\"label\"]\n",
    "    emotion = max(emotion_pipeline(text)[0], key=lambda x: x[\"score\"])[\"label\"]\n",
    "    return {\n",
    "        \"sentiment\": sentiment,\n",
    "        \"emotion\": emotion\n",
    "    }\n",
    "\n",
    "# ------------------- SPEECH-BASED EMOTION -------------------\n",
    "\n",
    "def analyze_speech_emotion(audio_path):\n",
    "    waveform, _ = librosa.load(audio_path, sr=16000)\n",
    "    inputs = feature_extractor(waveform, sampling_rate=16000, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = speech_model(**inputs).logits\n",
    "        emotion_index = torch.argmax(logits).item()\n",
    "\n",
    "    emotions = [\"Neutral\", \"Happy\", \"Sad\", \"Angry\", \"Fear\"]\n",
    "    return emotions[emotion_index]\n",
    "\n",
    "# ------------------- EMOTION FUSION -------------------\n",
    "\n",
    "def fuse_emotions(speech_emotion, text_emotion):\n",
    "    scale = {\"Happy\": 1, \"Neutral\": 0, \"Sad\": -1, \"Angry\": -2, \"Fear\": -3}\n",
    "    avg = (scale[speech_emotion] + scale[text_emotion]) / 2\n",
    "    final = min(scale, key=lambda x: abs(scale[x] - avg))\n",
    "    return final\n",
    "\n",
    "# ------------------- STEP 4: Emotion-Aware LLM Response -------------------\n",
    "\n",
    "import random\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ‚úÖ Optional TTS Synthesizer (Coqui or fallback)\n",
    "try:\n",
    "    tts_model = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False, gpu=DEVICE == \"cuda\")\n",
    "    print(\"‚úÖ Coqui TTS loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è TTS load failed: {e}\")\n",
    "    tts_model = None\n",
    "\n",
    "# ------------------- TTS OPTIONS HANDLING -------------------\n",
    "if USE_TTS:\n",
    "    try:\n",
    "        if not tts_model:\n",
    "            tts_model = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False, gpu=DEVICE == \"cuda\")\n",
    "        print(\"‚úÖ Coqui TTS confirmed active.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Coqui TTS failed: {e}\")\n",
    "        if USE_TTS_LITE:\n",
    "            import pyttsx3\n",
    "            tts_model = pyttsx3.init()\n",
    "            print(\"üîÅ Using pyttsx3 fallback.\")\n",
    "        elif USE_TTS_ALT:\n",
    "            from gtts import gTTS\n",
    "            tts_model = \"gTTS\"\n",
    "            print(\"üîÅ Using gTTS fallback.\")\n",
    "        else:\n",
    "            tts_model = None\n",
    "            print(\"‚ö†Ô∏è No TTS active. Only text replies will work.\")\n",
    "else:\n",
    "    tts_model = None\n",
    "    print(\"‚õî TTS globally disabled.\")\n",
    "\n",
    "# ------------------- Prompt Templates -------------------\n",
    "EMOTION_TEMPLATES = {\n",
    "    \"Happy\": \"The user sounds cheerful and happy. Respond positively and engagingly.\",\n",
    "    \"Neutral\": \"The user is calm and neutral. Respond helpfully and politely.\",\n",
    "    \"Sad\": \"The user sounds sad. Be empathetic and supportive in your reply.\",\n",
    "    \"Angry\": \"The user sounds angry or frustrated. Respond with calmness and offer solutions.\",\n",
    "    \"Fear\": \"The user seems anxious or fearful. Reassure and guide gently.\"\n",
    "}\n",
    "\n",
    "# ------------------- Generator Function -------------------\n",
    "def generate_emotion_aware_reply(text, fused_emotion, model_choice=\"hermes\", use_tts=False):\n",
    "    emotion_prefix = EMOTION_TEMPLATES.get(fused_emotion, \"The user has expressed something.\")\n",
    "    prompt = f\"{emotion_prefix}\\n\\nUser said:\\n{text.strip()}\\n\\nYour response:\"\n",
    "\n",
    "    if model_choice == \"hermes\":\n",
    "        tokenizer = ACTIVE_MODELS[\"hermes\"][\"tokenizer\"]\n",
    "        model = ACTIVE_MODELS[\"hermes\"][\"model\"]\n",
    "    elif model_choice == \"falcon3\":\n",
    "        tokenizer = ACTIVE_MODELS[\"falcon3\"][\"tokenizer\"]\n",
    "        model = ACTIVE_MODELS[\"falcon3\"][\"model\"]\n",
    "    elif model_choice == \"rag\":\n",
    "        tokenizer = ACTIVE_MODELS[\"rag\"][\"tokenizer\"]\n",
    "        retriever = ACTIVE_MODELS[\"rag\"][\"retriever\"]\n",
    "        model = ACTIVE_MODELS[\"rag\"][\"model\"]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model_choice. Choose from: mistral, falcon3, rag\")\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "    reply = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    result = {\n",
    "        \"prompt\": prompt,\n",
    "        \"reply\": reply,\n",
    "        \"emotion\": fused_emotion\n",
    "    }\n",
    "\n",
    "    if use_tts and tts_model:\n",
    "        audio_path = f\"tts_reply_{random.randint(1000, 9999)}.wav\"\n",
    "\n",
    "        if USE_TTS_LITE and not isinstance(tts_model, str):\n",
    "            tts_model.save_to_file(reply, audio_path)\n",
    "            tts_model.runAndWait()\n",
    "        elif USE_TTS_ALT and tts_model == \"gTTS\":\n",
    "            gtts_obj = gTTS(text=reply)\n",
    "            gtts_obj.save(audio_path)\n",
    "        elif not USE_TTS_ALT and not USE_TTS_LITE:\n",
    "            tts_model.tts_to_file(text=reply, file_path=audio_path)\n",
    "\n",
    "        result[\"audio_path\"] = audio_path\n",
    "\n",
    "    return result\n",
    "\n",
    "# ------------------- TEST HARNESS: CLI PIPELINE -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"sample_audio.wav\"  # Must be 16kHz mono\n",
    "    user_input = \"I‚Äôm feeling completely drained today...\"\n",
    "\n",
    "    print(\"\\nüîÑ Preprocessing input...\")\n",
    "    processed_text = preprocess_text(user_input)\n",
    "    sentiment_result = analyze_text_sentiment(processed_text[\"cleaned_text\"])\n",
    "    speech_emotion = analyze_speech_emotion(audio_file)\n",
    "\n",
    "    fused_emotion = fuse_emotions(speech_emotion, sentiment_result[\"emotion\"])\n",
    "    print(f\"üß† Fused Emotion: {fused_emotion}\")\n",
    "\n",
    "    print(\"\\nü§ñ Generating response using Hermes (top-ranked)...\")\n",
    "    reply = generate_emotion_aware_reply(\n",
    "        text=processed_text[\"cleaned_text\"],\n",
    "        fused_emotion=fused_emotion,\n",
    "        model_choice=\"hermes\",\n",
    "        use_tts=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚úÖ Final Reply:\", reply[\"reply\"])\n",
    "    if \"audio_path\" in reply:\n",
    "        print(f\"üéß TTS Output saved to: {reply['audio_path']}\")\n",
    "\n",
    "# ------------------- GRADIO INTERFACE -------------------\n",
    "import gradio as gr\n",
    "\n",
    "def full_pipeline(audio_file, user_text, model_choice=\"hermes\"):\n",
    "    processed_text = preprocess_text(user_text)\n",
    "    sentiment = analyze_text_sentiment(processed_text[\"cleaned_text\"])\n",
    "    speech_emotion = analyze_speech_emotion(audio_file.name)\n",
    "\n",
    "    fused = fuse_emotions(speech_emotion, sentiment[\"emotion\"])\n",
    "\n",
    "    reply = generate_emotion_aware_reply(\n",
    "        text=processed_text[\"cleaned_text\"],\n",
    "        fused_emotion=fused,\n",
    "        model_choice=model_choice,\n",
    "        use_tts=True\n",
    "    )\n",
    "\n",
    "    return reply[\"reply\"], fused, reply.get(\"audio_path\", None)\n",
    "\n",
    "gr.Interface(\n",
    "    fn=full_pipeline,\n",
    "    inputs=[\n",
    "        gr.Audio(type=\"file\", label=\"üé§ Upload Audio (16kHz WAV)\"),\n",
    "        gr.Textbox(label=\"üìù Your Text\"),\n",
    "        gr.Radio(choices=[\"hermes\", \"falcon3\", \"rag\"], value=\"hermes\", label=\"üí¨ Choose LLM\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"ü§ñ AI Reply\"),\n",
    "        gr.Textbox(label=\"üß† Fused Emotion\"),\n",
    "        gr.Audio(label=\"üéß TTS Output\")\n",
    "    ],\n",
    "    title=\"üéôÔ∏è Emotion-Aware AI Assistant\",\n",
    "    live=True\n",
    ").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca8716f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
